{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "id": "PRlPRYhoXYLf"
      },
      "outputs": [],
      "source": [
        "# pip install llama-index-llms-huggingface-api llama-index-embeddings-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install python-dotenv"
      ],
      "metadata": {
        "id": "wX5i6ZrzYNhb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install llama-index-llms-groq"
      ],
      "metadata": {
        "id": "h34X3LtiYwsB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install llama-index-vector-stores-chroma"
      ],
      "metadata": {
        "id": "7OrgE0cbrLHN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install llama-index-tools-google"
      ],
      "metadata": {
        "id": "TLko5Fgmr7i4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install chromadb"
      ],
      "metadata": {
        "id": "mHtjF0Z9FqYL"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple setup"
      ],
      "metadata": {
        "id": "SiegzAacaMEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.groq import Groq\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "llm = Groq(model=\"llama3-70b-8192\", api_key=userdata.get('groq_api'))\n",
        "\n",
        "response = llm.complete(\"Hello, how are you?\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5aJMXwGXeMX",
        "outputId": "3153e087-240e-4754-edca-20913f056ba2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm just a language model, I don't have feelings or emotions like humans do, so I don't have good or bad days. However, I'm functioning properly and ready to assist you with any questions or tasks you may have! How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialising Agents\n"
      ],
      "metadata": {
        "id": "ttgQotd09WrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "from llama_index.core.agent.workflow import AgentWorkflow\n",
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "# define sample Tool -- type annotations, function names, and docstrings, are all included in parsed schemas!\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiplies two integers and returns the resulting integer\"\"\"\n",
        "    return a * b\n",
        "\n",
        "# initialize agent\n",
        "agent = AgentWorkflow.from_tools_or_functions(\n",
        "    [FunctionTool.from_defaults(multiply)],\n",
        "    llm=llm\n",
        ")"
      ],
      "metadata": {
        "id": "H_kdwZhL7GRe"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = await agent.run(\"What is 2 times 2?\")"
      ],
      "metadata": {
        "id": "y60z53Oi9dCn"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dC7Nao5L9hfA",
        "outputId": "bc9cdf29-e30a-4b4a-f50b-2248a3a0e952"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The answer is 4.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agents are stateless by default**, add remembering past interactions is opt-in using a Context object This might be useful if you want to use an agent that needs to remember previous interactions, like a chatbot that maintains context across multiple messages or a task manager that needs to track progress over time."
      ],
      "metadata": {
        "id": "wky3dn_M9t_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.workflow import Context"
      ],
      "metadata": {
        "id": "YizjTM7M9kch"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ctx= Context(agent)"
      ],
      "metadata": {
        "id": "-TCpcREJ-Bx3"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = await agent.run(\"My name is Bob.\", ctx=ctx)\n",
        "response = await agent.run(\"What was my name again?\", ctx=ctx)"
      ],
      "metadata": {
        "id": "52bgxupD-F8o"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGPrWwVZ-ISq",
        "outputId": "9604160f-c642-4549-a9d1-39dc64ec48df"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your name is Bob.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "\n",
        "async def fetch_data(delay):\n",
        "    print(f\"Started fetching data with {delay}s delay\")\n",
        "\n",
        "    # Simulates I/O-bound work, such as network operation\n",
        "    await asyncio.sleep(0.1)\n",
        "\n",
        "    print(\"Finished fetching data\")\n",
        "    return f\"Data after {delay}s\"\n",
        "\n",
        "\n",
        "async def main():\n",
        "    print(\"Starting main\")\n",
        "\n",
        "    # Schedule two tasks concurrently\n",
        "    task1 = asyncio.create_task(fetch_data(2))\n",
        "    task2 = asyncio.create_task(fetch_data(3))\n",
        "\n",
        "    # Wait until both tasks complete\n",
        "    result1, result2 = await asyncio.gather(task1, task2)\n",
        "\n",
        "    print(result1)\n",
        "    print(result2)\n",
        "    print(\"Main complete\")"
      ],
      "metadata": {
        "id": "eYNoivSl_GR3"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaHKcfY0-MXx",
        "outputId": "bcabf336-73b9-4ef9-f36f-ec2df4484840"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting main\n",
            "Started fetching data with 2s delay\n",
            "Started fetching data with 3s delay\n",
            "Finished fetching data\n",
            "Finished fetching data\n",
            "Data after 2s\n",
            "Data after 3s\n",
            "Main complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating RAG Agents with QueryEngineTools"
      ],
      "metadata": {
        "id": "6mXRJimZEyGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "import chromadb\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "db = chromadb.PersistentClient(path=\"./alfred_chroma_db\")\n",
        "chroma_collection = db.get_or_create_collection(\"alfred\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "\n",
        "index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model)"
      ],
      "metadata": {
        "id": "zRsj06lA_nm3"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "\n",
        "pipeline = IngestionPipeline(\n",
        "    transformations=[\n",
        "        SentenceSplitter(chunk_size=25, chunk_overlap=0),\n",
        "        HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\"),\n",
        "    ],\n",
        "    vector_store=vector_store,\n",
        ")"
      ],
      "metadata": {
        "id": "7rcAx34AE2lZ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "reader = SimpleDirectoryReader(input_dir=\"./directory\")\n",
        "documents = reader.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksH5M-3DFM_T",
        "outputId": "fece022c-3a70-44bc-af8a-3d58073002ac"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:llama_index.core.readers.file.base:`llama-index-readers-file` package not found, some file readers will not be available if not provided by the `file_extractor` parameter.\n",
            "WARNING:llama_index.core.readers.file.base:`llama-index-readers-file` package not found, some file readers will not be available if not provided by the `file_extractor` parameter.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nodes = await pipeline.arun(documents=documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eg3EKQZQGGTZ",
        "outputId": "e944188e-cd1e-4aef-a58b-6b34c0bbebf9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata length (13) is close to chunk size (25). Resulting chunks are less than 50 tokens. Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n",
            "Metadata length (13) is close to chunk size (25). Resulting chunks are less than 50 tokens. Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "query_engine = index.as_query_engine(llm=llm, similarity_top_k=3) # as shown in the Components in LlamaIndex section\n",
        "\n",
        "query_engine_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=query_engine,\n",
        "    name=\"name\",\n",
        "    description=\"a specific description\",\n",
        "    return_direct=False,\n",
        ")\n",
        "query_engine_agent = AgentWorkflow.from_tools_or_functions(\n",
        "    [query_engine_tool],\n",
        "    llm=llm,\n",
        "    system_prompt=\"You are a helpful assistant that has access to a database containing cpmputer networks notes. \"\n",
        ")"
      ],
      "metadata": {
        "id": "k-XljrucGLIt"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Multi-agent systems\n"
      ],
      "metadata": {
        "id": "53LUBryNHJ2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent.workflow import (\n",
        "    AgentWorkflow,\n",
        "    FunctionAgent,\n",
        "    ReActAgent,\n",
        ")\n",
        "\n",
        "# Define some tools\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two numbers.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "def subtract(a: int, b: int) -> int:\n",
        "    \"\"\"Subtract two numbers.\"\"\"\n",
        "    return a - b\n",
        "\n",
        "\n",
        "# Create agent configs\n",
        "# NOTE: we can use FunctionAgent or ReActAgent here.\n",
        "# FunctionAgent works for LLMs with a function calling API.\n",
        "# ReActAgent works for any LLM.\n",
        "calculator_agent = ReActAgent(\n",
        "    name=\"calculator\",\n",
        "    description=\"Performs basic arithmetic operations\",\n",
        "    system_prompt=\"You are a calculator assistant. Use your tools for any math operation.\",\n",
        "    tools=[add, subtract],\n",
        "    llm=llm,\n",
        ")\n",
        "\n",
        "query_agent = ReActAgent(\n",
        "    name=\"info_lookup\",\n",
        "    description=\"Looks up information about XYZ\",\n",
        "    system_prompt=\"Use your tool to query a RAG system to answer information about XYZ\",\n",
        "    tools=[query_engine_tool],\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "# Create and run the workflow\n",
        "agent = AgentWorkflow(\n",
        "    agents=[calculator_agent, query_agent], root_agent=\"calculator\"\n",
        ")\n",
        "\n",
        "# Run the system\n",
        "response = await agent.run(user_msg=\"Can you add 5 and 3?\")"
      ],
      "metadata": {
        "id": "Yteggo35HHXW"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpJlZbXtHcbq",
        "outputId": "872b7fc8-df9c-472a-a7cf-e71dfa996d12"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AgentOutput(response=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='The result of adding 5 and 3 is 8.')]), tool_calls=[ToolCallResult(tool_name='add', tool_kwargs={'a': 5, 'b': 3}, tool_id='a78e5ef5-d11f-4aa8-b67b-b25fa3a49db4', tool_output=ToolOutput(content='8', tool_name='add', raw_input={'args': (), 'kwargs': {'a': 5, 'b': 3}}, raw_output=8, is_error=False), return_direct=False)], raw={'id': 'chatcmpl-e79de6c5-abed-4d97-9f31-50f73d455398', 'choices': [{'delta': {'content': None, 'function_call': None, 'refusal': None, 'role': None, 'tool_calls': None}, 'finish_reason': 'stop', 'index': 0, 'logprobs': None}], 'created': 1745436396, 'model': 'llama3-70b-8192', 'object': 'chat.completion.chunk', 'service_tier': None, 'system_fingerprint': 'fp_dd4ae1c591', 'usage': None, 'x_groq': {'id': 'req_01jsj1vjxjenmbxacy4c3x3qqf', 'usage': {'queue_time': 0.22921458400000003, 'prompt_tokens': 763, 'prompt_time': 0.025127025, 'completion_tokens': 37, 'completion_time': 0.105714286, 'total_tokens': 800, 'total_time': 0.130841311}}}, current_agent_name='calculator')"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GGpaKoU8HgSS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
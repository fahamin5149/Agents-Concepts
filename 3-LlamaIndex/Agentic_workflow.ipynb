{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fmSNECmx50jn"
      },
      "outputs": [],
      "source": [
        "# !pip install llama-index-utils-workflow\n",
        "# !pip install llama-index-llms-huggingface-api llama-index-embeddings-huggingface\n",
        "# !pip install python-dotenv\n",
        "# !pip install llama-index-llms-groq\n",
        "# !pip install llama-index-vector-stores-chroma\n",
        "# !pip install llama-index-tools-google"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple setup"
      ],
      "metadata": {
        "id": "SiegzAacaMEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.groq import Groq\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "llm = Groq(model=\"llama3-70b-8192\", api_key=userdata.get('groq_api'))\n",
        "\n",
        "response = llm.complete(\"Hello, how are you?\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5aJMXwGXeMX",
        "outputId": "97584e84-fdef-4b83-f1c9-bddcb107e1d1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm just a language model, I don't have emotions or feelings like humans do, so I don't have good or bad days. However, I'm functioning properly and ready to assist you with any questions or tasks you may have! How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Workflow Creation\n"
      ],
      "metadata": {
        "id": "i-RL4XTRKkaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.workflow import StartEvent, StopEvent, Workflow, step\n",
        "\n",
        "class MyWorkflow(Workflow):\n",
        "  @step\n",
        "  async def my_step(self, ev: StartEvent)->StopEvent:\n",
        "    return StopEvent(result=\"Hello, world!\")\n",
        "\n",
        "\n",
        "w = MyWorkflow(timeout=10, verbose=False)\n",
        "result = await w.run()"
      ],
      "metadata": {
        "id": "wX5i6ZrzYNhb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "h34X3LtiYwsB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "4c5d5f64-3e21-4ec3-b0bd-634c2ef20dd1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello, world!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connecting Multiple Steps\n"
      ],
      "metadata": {
        "id": "BaWPa5fBKnA8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To connect multiple steps, we create custom events that carry data between steps. To do so, we need to add an Event that is passed between the steps and transfers the output of the first step to the second step.\n",
        "\n"
      ],
      "metadata": {
        "id": "d9wZO5EGLqmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.workflow import Event\n",
        "\n",
        "class ProcessingEvent(Event):\n",
        "    intermediate_result: str\n",
        "\n",
        "class MultiStepWorkflow(Workflow):\n",
        "    @step\n",
        "    async def step_one(self, ev: StartEvent) -> ProcessingEvent:\n",
        "        # Process initial data\n",
        "        return ProcessingEvent(intermediate_result=\"Step 1 complete\")\n",
        "\n",
        "    @step\n",
        "    async def step_three(self, ev: ProcessingEvent) -> ProcessingEvent:\n",
        "        # Process initial data\n",
        "        return ProcessingEvent(intermediate_result=\"Step 2 complete\")\n",
        "\n",
        "\n",
        "    @step\n",
        "    async def step_two(self, ev: ProcessingEvent) -> StopEvent:\n",
        "        # Use the intermediate result\n",
        "        final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
        "        return StopEvent(result=final_result)\n",
        "\n",
        "w = MultiStepWorkflow(timeout=10, verbose=False)\n",
        "result = await w.run()\n",
        "result"
      ],
      "metadata": {
        "id": "7OrgE0cbrLHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.workflow import Event\n",
        "\n",
        "class ProcessingEvent(Event):\n",
        "    intermediate_result: str\n",
        "\n",
        "class MultiStepWorkflow(Workflow):\n",
        "    @step\n",
        "    async def step_one(self, ev: StartEvent) -> ProcessingEvent:\n",
        "        # Process initial data\n",
        "        return ProcessingEvent(intermediate_result=\"Step 1 complete\")\n",
        "\n",
        "    @step\n",
        "    async def step_two(self, ev: ProcessingEvent) -> StopEvent:\n",
        "        # Use the intermediate result\n",
        "        final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
        "        return StopEvent(result=final_result)\n",
        "\n",
        "w = MultiStepWorkflow(timeout=10, verbose=False)\n",
        "result = await w.run()\n",
        "result"
      ],
      "metadata": {
        "id": "TLko5Fgmr7i4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oFswHjgxMR2x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
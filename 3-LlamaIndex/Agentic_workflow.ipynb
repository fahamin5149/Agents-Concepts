{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fmSNECmx50jn"
      },
      "outputs": [],
      "source": [
        "# !pip install llama-index-utils-workflow\n",
        "# !pip install llama-index-llms-huggingface-api llama-index-embeddings-huggingface\n",
        "# !pip install python-dotenv\n",
        "# !pip install llama-index-llms-groq\n",
        "# !pip install llama-index-vector-stores-chroma\n",
        "# !pip install llama-index-tools-google"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple setup"
      ],
      "metadata": {
        "id": "SiegzAacaMEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.groq import Groq\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "llm = Groq(model=\"llama3-70b-8192\", api_key=userdata.get('groq_api'))\n",
        "\n",
        "response = llm.complete(\"Hello, how are you?\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5aJMXwGXeMX",
        "outputId": "272153eb-986a-41bc-93e8-c86191ac2b88"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm just a language model, I don't have emotions or feelings like humans do, so I don't have good or bad days. However, I'm functioning properly and ready to assist you with any questions or tasks you may have! How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Workflow Creation\n"
      ],
      "metadata": {
        "id": "i-RL4XTRKkaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.workflow import StartEvent, StopEvent, Workflow, step\n",
        "\n",
        "class MyWorkflow(Workflow):\n",
        "  @step\n",
        "  async def my_step(self, ev: StartEvent)->StopEvent:\n",
        "    return StopEvent(result=\"Hello, world!\")\n",
        "\n",
        "\n",
        "w = MyWorkflow(timeout=10, verbose=False)\n",
        "result = await w.run()"
      ],
      "metadata": {
        "id": "wX5i6ZrzYNhb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "h34X3LtiYwsB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "90d10239-2850-4714-f449-3bc2f99db9dc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello, world!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connecting Multiple Steps\n"
      ],
      "metadata": {
        "id": "BaWPa5fBKnA8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To connect multiple steps, we create custom events that carry data between steps. To do so, we need to add an Event that is passed between the steps and transfers the output of the first step to the second step.\n",
        "\n"
      ],
      "metadata": {
        "id": "d9wZO5EGLqmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.workflow import Event\n",
        "\n",
        "class ProcessingEvent(Event):\n",
        "    intermediate_result: str\n",
        "\n",
        "class MultiStepWorkflow(Workflow):\n",
        "    @step\n",
        "    async def step_one(self, ev: StartEvent) -> ProcessingEvent:\n",
        "        # Process initial data\n",
        "        return ProcessingEvent(intermediate_result=\"Step 1 complete\")\n",
        "\n",
        "    @step\n",
        "    async def step_two(self, ev: ProcessingEvent) -> StopEvent:\n",
        "        # Use the intermediate result\n",
        "        final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
        "        return StopEvent(result=final_result)\n",
        "\n",
        "w = MultiStepWorkflow(timeout=10, verbose=False)\n",
        "result = await w.run()\n",
        "result"
      ],
      "metadata": {
        "id": "7OrgE0cbrLHN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "d641666b-75a5-4599-cf3a-3ecbdcbc4dee"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Finished processing: Step 1 complete'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.workflow import Event\n",
        "\n",
        "class ProcessingEvent(Event):\n",
        "    intermediate_result: str\n",
        "\n",
        "class MultiStepWorkflow(Workflow):\n",
        "    @step\n",
        "    async def step_one(self, ev: StartEvent) -> ProcessingEvent:\n",
        "        # Process initial data\n",
        "        return ProcessingEvent(intermediate_result=\"Step 1 complete\")\n",
        "\n",
        "    @step\n",
        "    async def step_two(self, ev: ProcessingEvent) -> StopEvent:\n",
        "        # Use the intermediate result\n",
        "        final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
        "        return StopEvent(result=final_result)\n",
        "\n",
        "w = MultiStepWorkflow(timeout=10, verbose=False)\n",
        "result = await w.run()\n",
        "result"
      ],
      "metadata": {
        "id": "TLko5Fgmr7i4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loops and Branches\n",
        "\n",
        "The type hinting is the most powerful part of workflows because it allows us to create branches, loops, and joins to facilitate more complex workflows.\n",
        "\n"
      ],
      "metadata": {
        "id": "rANyVIi1E7d2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "### Visual Summary of Flow:\n",
        "1. `step_one(StartEvent)` → `LoopEvent` → rerun `step_one`\n",
        "2. `step_one(LoopEvent)` → maybe `LoopEvent` again → rerun\n",
        "3. Eventually `step_one(...)` → `ProcessingEvent` → triggers `step_two`"
      ],
      "metadata": {
        "id": "qlZYh3LjHnxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.workflow import Event\n",
        "import random\n",
        "\n",
        "class ProcessingEvent(Event):\n",
        "  intermediate_result: str\n",
        "\n",
        "class LoopEvent(Event):\n",
        "  loop_output: str\n",
        "\n",
        "class MultiStepWorkflow(Workflow):\n",
        "  @step\n",
        "  async def step_one(self, ev: StartEvent | LoopEvent) -> ProcessingEvent | LoopEvent:\n",
        "    if random.randint(0,1)==0:\n",
        "      \"\"\"\n",
        "      So when step_one returns a LoopEvent, the workflow framework routes the returned\n",
        "      event back to the appropriate handler, which in this case is again step_one,\n",
        "      because step_one accepts LoopEvent as input.\n",
        "\n",
        "      This essentially creates a loop until a ProcessingEvent is returned, which then\n",
        "      gets routed to step_two.\n",
        "\n",
        "\n",
        "      \"\"\"\n",
        "      print(\"bad thing happed\")\n",
        "      return LoopEvent(loop_output=\"back to step one.\")\n",
        "\n",
        "    else:\n",
        "      print(\"good thing happened\")\n",
        "      return ProcessingEvent(intermediate_result=\"Step 1 complete\")\n",
        "\n",
        "  @step\n",
        "  async def step_two(self, ev: ProcessingEvent) -> StopEvent:\n",
        "    final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
        "    return StopEvent(result=final_result)"
      ],
      "metadata": {
        "id": "oFswHjgxMR2x"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = MultiStepWorkflow(verbose=False)\n",
        "result = await w.run()\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "yQ_Amd9mGJ3n",
        "outputId": "090b1b1d-a9ab-4c80-e380-1a851af74ce6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bad thing happed\n",
            "bad thing happed\n",
            "bad thing happed\n",
            "bad thing happed\n",
            "bad thing happed\n",
            "bad thing happed\n",
            "good thing happened\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Finished processing: Step 1 complete'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drawing Workflows"
      ],
      "metadata": {
        "id": "hw5Cpq6FH2Mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.utils.workflow import draw_all_possible_flows\n",
        "\n",
        "draw_all_possible_flows(w, \"flow.html\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKx277CgGLm_",
        "outputId": "caea39d9-32b2-4cf3-f967-a0d51987a9af"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flow.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## State Management\n"
      ],
      "metadata": {
        "id": "zSnMUXa7H-3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "State management is useful when you want to keep track of the state of the workflow, so that every step has access to the same state. We can do this by using the Context type hint on top of a parameter in the step function.\n",
        "\n"
      ],
      "metadata": {
        "id": "E15ltg01Iohl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.workflow import Context, StartEvent, StopEvent\n",
        "\n",
        "\n",
        "@step\n",
        "async def query(self, ctx: Context, ev: StartEvent) -> StopEvent:\n",
        "    # store query in the context\n",
        "    await ctx.set(\"query\", \"What is the capital of France?\")\n",
        "\n",
        "    # do something with context and event\n",
        "    val = ...\n",
        "\n",
        "    # retrieve query from the context\n",
        "    query = await ctx.get(\"query\")\n",
        "\n",
        "    return StopEvent(result=val)"
      ],
      "metadata": {
        "id": "fB6ZN6_PH5X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automating workflows with Multi-Agent Workflows\n"
      ],
      "metadata": {
        "id": "aia4PRnTItJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent.workflow import AgentWorkflow, ReActAgent\n",
        "\n",
        "# Define some tools\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two numbers.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply two numbers.\"\"\"\n",
        "    return a * b"
      ],
      "metadata": {
        "id": "0Vq5rl4tIqWy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8E3rzpXJu3I",
        "outputId": "f4cac3c4-38c0-46ae-e77f-a0174b0a1065"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Groq(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7e1e99647910>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x7e1f79ebfce0>, completion_to_prompt=<function default_completion_to_prompt at 0x7e1f79a1de40>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, model='llama3-70b-8192', temperature=0.1, max_tokens=None, logprobs=None, top_logprobs=0, additional_kwargs={}, max_retries=3, timeout=60.0, default_headers=None, reuse_client=True, api_key='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', api_base='https://api.groq.com/openai/v1', api_version='', strict=False, reasoning_effort=None, modalities=None, audio_config=None, context_window=3900, is_chat_model=True, is_function_calling_model=True, tokenizer=None)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can pass functions directly without FunctionTool -- the fn/docstring are parsed for the name/description\n"
      ],
      "metadata": {
        "id": "XkRS0bmIJ5_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multiply_agent= ReActAgent(\n",
        "    name=\"multiply_agent\",\n",
        "    description=\"Is able to multiply two integers\",\n",
        "    system_prompt=\"A helpful assistant that can use a tool to multiply numbers.\",\n",
        "    tools=[multiply],\n",
        "    llm=llm,\n",
        ")"
      ],
      "metadata": {
        "id": "k-XlTjfBJxP6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "addition_agent = ReActAgent(\n",
        "    name=\"add_agent\",\n",
        "    description=\"Is able to add two integers\",\n",
        "    system_prompt=\"A helpful assistant that can use a tool to add numbers.\",\n",
        "    tools=[add],\n",
        "    llm=llm,\n",
        ")"
      ],
      "metadata": {
        "id": "5IlR92aMKJMu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the workflow\n",
        "workflow = AgentWorkflow(\n",
        "    agents=[multiply_agent, addition_agent],\n",
        "    root_agent=\"multiply_agent\",\n",
        ")"
      ],
      "metadata": {
        "id": "2T4QdETFKKyO"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = await workflow.run(user_msg=\"Can you add 5 and 3?\")\n"
      ],
      "metadata": {
        "id": "14m2qUPGKNoM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjDSaqr5KO6c",
        "outputId": "5cd5ae4b-993b-406d-a9b0-b5901f188fbe"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "await workflow.run(user_msg=\"Can you add 5 and 3 and then multipy by 4*5?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpyXQhdIKRJB",
        "outputId": "487d6732-4723-4eca-f7c3-b2188d628614"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AgentOutput(response=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='The result of adding 5 and 3 and then multiplying by 4*5 is 160.')]), tool_calls=[ToolCallResult(tool_name='handoff', tool_kwargs={'to_agent': 'add_agent', 'reason': 'need to add 5 and 3'}, tool_id='b5f4fb65-4cb0-4956-b007-69c97c0e98fe', tool_output=ToolOutput(content='Agent add_agent is now handling the request due to the following reason: need to add 5 and 3.\\nPlease continue with the current request.', tool_name='handoff', raw_input={'args': (), 'kwargs': {'to_agent': 'add_agent', 'reason': 'need to add 5 and 3'}}, raw_output='Agent add_agent is now handling the request due to the following reason: need to add 5 and 3.\\nPlease continue with the current request.', is_error=False), return_direct=True), ToolCallResult(tool_name='add', tool_kwargs={'a': 5, 'b': 3}, tool_id='7848267d-c637-4036-8190-61e2c0dc09f5', tool_output=ToolOutput(content='8', tool_name='add', raw_input={'args': (), 'kwargs': {'a': 5, 'b': 3}}, raw_output=8, is_error=False), return_direct=False), ToolCallResult(tool_name='handoff', tool_kwargs={'to_agent': 'multiply_agent', 'reason': 'need to multiply 4 and 5'}, tool_id='df4b0740-a6df-4fac-96f0-6eb81ecddec4', tool_output=ToolOutput(content='Agent multiply_agent is now handling the request due to the following reason: need to multiply 4 and 5.\\nPlease continue with the current request.', tool_name='handoff', raw_input={'args': (), 'kwargs': {'to_agent': 'multiply_agent', 'reason': 'need to multiply 4 and 5'}}, raw_output='Agent multiply_agent is now handling the request due to the following reason: need to multiply 4 and 5.\\nPlease continue with the current request.', is_error=False), return_direct=True), ToolCallResult(tool_name='multiply', tool_kwargs={'a': 4, 'b': 5}, tool_id='8d770096-6bd8-4b98-8775-cf51a14d60d7', tool_output=ToolOutput(content='20', tool_name='multiply', raw_input={'args': (), 'kwargs': {'a': 4, 'b': 5}}, raw_output=20, is_error=False), return_direct=False), ToolCallResult(tool_name='multiply', tool_kwargs={'a': 8, 'b': 20}, tool_id='68dc8bb4-e328-4c19-b2da-b3c41266e71a', tool_output=ToolOutput(content='160', tool_name='multiply', raw_input={'args': (), 'kwargs': {'a': 8, 'b': 20}}, raw_output=160, is_error=False), return_direct=False)], raw={'id': 'chatcmpl-bb663e40-0e01-4635-aabb-fadef9f0be1c', 'choices': [{'delta': {'content': None, 'function_call': None, 'refusal': None, 'role': None, 'tool_calls': None}, 'finish_reason': 'stop', 'index': 0, 'logprobs': None}], 'created': 1745604939, 'model': 'llama3-70b-8192', 'object': 'chat.completion.chunk', 'service_tier': None, 'system_fingerprint': 'fp_dd4ae1c591', 'usage': None, 'x_groq': {'id': 'req_01jsq2k3hsfseseee9spv3y7an', 'usage': {'queue_time': 0.22899731900000003, 'prompt_tokens': 1167, 'prompt_time': 0.045943998, 'completion_tokens': 45, 'completion_time': 0.151157205, 'total_tokens': 1212, 'total_time': 0.197101203}}}, current_agent_name='multiply_agent')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response=await workflow.run(user_msg=\"Can you add 5 and 3 and then multipy by 4*5?\")\n"
      ],
      "metadata": {
        "id": "-MJCAmpzKXdS"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KlsKpM4KgUu",
        "outputId": "8b8a10cc-5a36-429e-df0d-73a2f559cf68"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The result of adding 5 and 3 and then multiplying by 4*5 is 160.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.workflow import Context\n",
        "\n",
        "# Define some tools\n",
        "async def add(ctx: Context, a: int, b: int) -> int:\n",
        "    \"\"\"Add two numbers.\"\"\"\n",
        "    # update our count\n",
        "    cur_state = await ctx.get(\"state\")\n",
        "    cur_state[\"num_fn_calls\"] += 1\n",
        "    await ctx.set(\"state\", cur_state)\n",
        "\n",
        "    return a + b\n",
        "\n",
        "async def multiply(ctx: Context, a: int, b: int) -> int:\n",
        "    \"\"\"Multiply two numbers.\"\"\"\n",
        "    # update our count\n",
        "    cur_state = await ctx.get(\"state\")\n",
        "    cur_state[\"num_fn_calls\"] += 1\n",
        "    await ctx.set(\"state\", cur_state)\n",
        "\n",
        "    return a * b\n",
        "\n",
        "...\n",
        "\n",
        "workflow = AgentWorkflow(\n",
        "    agents=[multiply_agent, addition_agent],\n",
        "    root_agent=\"multiply_agent\",\n",
        "    initial_state={\"num_fn_calls\": 0},\n",
        "    state_prompt=\"Current state: {state}. User message: {msg}\",\n",
        ")\n",
        "\n",
        "# run the workflow with context\n",
        "ctx = Context(workflow)\n",
        "response = await workflow.run(user_msg=\"Can you add 5 and 3?\", ctx=ctx)\n",
        "\n",
        "# pull out and inspect the state\n",
        "state = await ctx.get(\"state\")\n",
        "print(state[\"num_fn_calls\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xy8vCjrLC1x",
        "outputId": "2a7eef59-8f66-4f88-8185-17732117daf1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hJtIIkAJLDL2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
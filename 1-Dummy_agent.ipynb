{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2db78b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "203082ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hf_token= os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f47985",
   "metadata": {},
   "source": [
    "### Serverless API\n",
    "\n",
    "In the Hugging Face ecosystem, there is convenient feature called Serverless API that allows you to easily run inference on many models. There's no installation or deployment required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06e9886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "os.environ[\"HF_TOKEN\"]= hf_token\n",
    "\n",
    "client = InferenceClient(\"meta-llama/Llama-3.2-3B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cccb871a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<InferenceClient(model='meta-llama/Llama-3.2-3B-Instruct', timeout=None)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ebfa8a",
   "metadata": {},
   "source": [
    "if we just do decoding, the model will only stop when it predicts an EOS token, and this does not happen here because this is a conversational (chat) model and we didn’t apply the chat template it expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "396d2ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of pakistan is  islamabad\n",
      "The capital of Pakistan is actually Islamabad, not Karachi or Lahore. Islamabad is a planned city located in the north of the country, and it has been the capital of Pakistan since 1959. It was chosen as the capital due to its strategic location and accessibility to the country's northern regions.\n",
      "\n",
      "Here are some interesting facts about Islamabad:\n",
      "\n",
      "1. **Planned city**: Islamabad was designed and built as a planned city in the 1960s, with the aim of creating a\n"
     ]
    }
   ],
   "source": [
    "output= client.text_generation(\n",
    "    \"The capital of pakistan is\",\n",
    "    max_new_tokens=100,\n",
    ")\n",
    "\n",
    "print(f\"The capital of pakistan is {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6ac393",
   "metadata": {},
   "source": [
    "If we now add the special tokens related to the Llama-3.2-3B-Instruct model that we’re using, the behavior changes and it now produces the expected EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48a3dd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "...Paris!\n"
     ]
    }
   ],
   "source": [
    "prompt=\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "The capital of France is<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "#<|start_header_id|>user<|end_header_id|>\n",
    "#<|eot_id|> signals the end of the user’s turn, allowing the model to determine where the assistant's response should start.\n",
    "\n",
    "output = client.text_generation(\n",
    "    prompt,\n",
    "    max_new_tokens=100,\n",
    ")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c871c2e",
   "metadata": {},
   "source": [
    "Using the `chat` method is a much more convenient and reliable way to apply chat templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94ba6a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "output= client.chat.completions.create(\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': 'The capital of France is'}\n",
    "    ],\n",
    "    stream=False,\n",
    "    max_tokens= 1024,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d324518e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionOutput(choices=[ChatCompletionOutputComplete(finish_reason='stop', index=0, message=ChatCompletionOutputMessage(role='assistant', content='Paris.', tool_calls=None), logprobs=None)], created=1742811095, id='', model='meta-llama/Llama-3.2-3B-Instruct', system_fingerprint='3.2.1-native', usage=ChatCompletionOutputUsage(completion_tokens=3, prompt_tokens=40, total_tokens=43), object='chat.completion')\n",
      "\n",
      "\n",
      "[ChatCompletionOutputComplete(finish_reason='stop', index=0, message=ChatCompletionOutputMessage(role='assistant', content='Paris.', tool_calls=None), logprobs=None)]\n",
      "\n",
      "\n",
      "ChatCompletionOutputComplete(finish_reason='stop', index=0, message=ChatCompletionOutputMessage(role='assistant', content='Paris.', tool_calls=None), logprobs=None)\n",
      "\n",
      "\n",
      "ChatCompletionOutputMessage(role='assistant', content='Paris.', tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "print(output)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(output.choices)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(output.choices[0])\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(output.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d6f4f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris.\n"
     ]
    }
   ],
   "source": [
    "print(output.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd503f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

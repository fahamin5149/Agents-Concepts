{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2a16e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hf_token= os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e957bde",
   "metadata": {},
   "source": [
    "### Serverless API\n",
    "\n",
    "In the Hugging Face ecosystem, there is convenient feature called Serverless API that allows you to easily run inference on many models. There's no installation or deployment required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "821c567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "os.environ[\"HF_TOKEN\"]= hf_token\n",
    "\n",
    "client = InferenceClient(\"meta-llama/Llama-3.2-3B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16b23e81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<InferenceClient(model='meta-llama/Llama-3.2-3B-Instruct', timeout=None)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024b11d2",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d50a0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ad3ecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer= AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3724a5b",
   "metadata": {},
   "source": [
    "## Agent Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14284a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install smolagents\n",
    "# import gradio as gr\n",
    "\n",
    "!pip install --upgrade gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1e2458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import CodeAgent, DuckDuckGoSearchTool, HfApiModel, load_tool, tool\n",
    "import datetime\n",
    "import requests\n",
    "import pytz\n",
    "import yaml\n",
    "from tools.final_answer import FinalAnswerTool\n",
    "\n",
    "from Gradio_UI import GradioUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ce8de96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is an example of a tool that does nothing. Amaze us with your creativity!\n",
    "@tool\n",
    "def my_custom_tool(arg1:str, arg2:int)-> str: # it's important to specify the return type\n",
    "    # Keep this format for the tool description / args description but feel free to modify the tool\n",
    "    \"\"\"A tool that does nothing yet \n",
    "    Args:\n",
    "        arg1: the first argument\n",
    "        arg2: the second argument\n",
    "    \"\"\"\n",
    "    return \"What magic will you build ?\"\n",
    "\n",
    "@tool\n",
    "def get_current_time_in_timezone(timezone: str) -> str:\n",
    "    \"\"\"A tool that fetches the current local time in a specified timezone.\n",
    "    Args:\n",
    "        timezone: A string representing a valid timezone (e.g., 'America/New_York').\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create timezone object\n",
    "        tz = pytz.timezone(timezone)\n",
    "        # Get current time in that timezone\n",
    "        local_time = datetime.datetime.now(tz).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        return f\"The current local time in {timezone} is: {local_time}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching time for timezone '{timezone}': {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9eb15ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073209d280354a769c711000cd942916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tool.py:   0%|          | 0.00/635 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\spaces--agents-course--text-to-image. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Chatbot.__init__() got an unexpected keyword argument 'resizeable'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 34\u001b[0m\n\u001b[0;32m     19\u001b[0m     prompt_templates \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(stream)\n\u001b[0;32m     21\u001b[0m agent \u001b[38;5;241m=\u001b[39m CodeAgent(\n\u001b[0;32m     22\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     23\u001b[0m     tools\u001b[38;5;241m=\u001b[39m[final_answer], \u001b[38;5;66;03m# add your tools here (don't remove final_answer)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     prompt_templates\u001b[38;5;241m=\u001b[39mprompt_templates \u001b[38;5;66;03m# Pass system prompt to CodeAgent\u001b[39;00m\n\u001b[0;32m     31\u001b[0m )\n\u001b[1;32m---> 34\u001b[0m GradioUI(agent)\u001b[38;5;241m.\u001b[39mlaunch()\n",
      "File \u001b[1;32m~\\Desktop\\Files\\FL\\Certifications\\3-Agents Hugging Face\\Agents-Concepts\\Gradio_UI.py:267\u001b[0m, in \u001b[0;36mGradioUI.launch\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    265\u001b[0m stored_messages \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mState([])\n\u001b[0;32m    266\u001b[0m file_uploads_log \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mState([])\n\u001b[1;32m--> 267\u001b[0m chatbot \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mChatbot(\n\u001b[0;32m    268\u001b[0m     label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgent\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    270\u001b[0m     avatar_images\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    271\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    272\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/datasets/agents-course/course-images/resolve/main/en/communication/Alfred.png\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    273\u001b[0m     ),\n\u001b[0;32m    274\u001b[0m     resizeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m     scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    276\u001b[0m )\n\u001b[0;32m    277\u001b[0m \u001b[38;5;66;03m# If an upload folder is provided, enable the upload feature\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_upload_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gradio\\component_meta.py:167\u001b[0m, in \u001b[0;36mupdateable.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: Chatbot.__init__() got an unexpected keyword argument 'resizeable'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\gradio\\analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.44.0, however version 4.44.1 is available, please upgrade. \n",
      "--------\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "final_answer = FinalAnswerTool()\n",
    "\n",
    "# If the agent does not answer, the model is overloaded, please use another model or the following Hugging Face Endpoint that also contains qwen2.5 coder:\n",
    "# model_id='https://pflgm2locj2t89co.us-east-1.aws.endpoints.huggingface.cloud' \n",
    "\n",
    "model = HfApiModel(\n",
    "max_tokens=2096,\n",
    "temperature=0.5,\n",
    "model_id='Qwen/Qwen2.5-Coder-32B-Instruct',# it is possible that this model may be overloaded\n",
    "custom_role_conversions=None,\n",
    ")\n",
    "\n",
    "\n",
    "# Import tool from Hub\n",
    "image_generation_tool = load_tool(\"agents-course/text-to-image\", trust_remote_code=True)\n",
    "\n",
    "# Load system prompt from prompt.yaml file\n",
    "with open(\"prompts.yaml\", 'r') as stream:\n",
    "    prompt_templates = yaml.safe_load(stream)\n",
    "    \n",
    "agent = CodeAgent(\n",
    "    model=model,\n",
    "    tools=[final_answer], # add your tools here (don't remove final_answer)\n",
    "    max_steps=6,\n",
    "    verbosity_level=1,\n",
    "    grammar=None,\n",
    "    planning_interval=None,\n",
    "    name=None,\n",
    "    description=None,\n",
    "    prompt_templates=prompt_templates # Pass system prompt to CodeAgent\n",
    ")\n",
    "\n",
    "\n",
    "GradioUI(agent).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846083f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2a16e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hf_token= os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e957bde",
   "metadata": {},
   "source": [
    "### Serverless API\n",
    "\n",
    "In the Hugging Face ecosystem, there is convenient feature called Serverless API that allows you to easily run inference on many models. There's no installation or deployment required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "821c567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "os.environ[\"HF_TOKEN\"]= hf_token\n",
    "\n",
    "client = InferenceClient(\"meta-llama/Llama-3.2-3B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16b23e81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<InferenceClient(model='meta-llama/Llama-3.2-3B-Instruct', timeout=None)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5066172",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d50a0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ad3ecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer= AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee83d7a",
   "metadata": {},
   "source": [
    "## Agent Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a835fe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install smolagents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "891f6786",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'smolagents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msmolagents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CodeAgent, DuckDuckGoSearchTool, HfApiModel, load_tool, tool\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'smolagents'"
     ]
    }
   ],
   "source": [
    "from smolagents import CodeAgent, DuckDuckGoSearchTool, HfApiModel, load_tool, tool\n",
    "import datetime\n",
    "import requests\n",
    "import pytz\n",
    "import yaml\n",
    "from tools.final_answer import FinalAnswerTool\n",
    "\n",
    "from Gradio_UI import GradioUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f785f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is an example of a tool that does nothing. Amaze us with your creativity!\n",
    "@tool\n",
    "def my_custom_tool(arg1:str, arg2:int)-> str: # it's important to specify the return type\n",
    "    # Keep this format for the tool description / args description but feel free to modify the tool\n",
    "    \"\"\"A tool that does nothing yet \n",
    "    Args:\n",
    "        arg1: the first argument\n",
    "        arg2: the second argument\n",
    "    \"\"\"\n",
    "    return \"What magic will you build ?\"\n",
    "\n",
    "@tool\n",
    "def get_current_time_in_timezone(timezone: str) -> str:\n",
    "    \"\"\"A tool that fetches the current local time in a specified timezone.\n",
    "    Args:\n",
    "        timezone: A string representing a valid timezone (e.g., 'America/New_York').\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create timezone object\n",
    "        tz = pytz.timezone(timezone)\n",
    "        # Get current time in that timezone\n",
    "        local_time = datetime.datetime.now(tz).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        return f\"The current local time in {timezone} is: {local_time}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching time for timezone '{timezone}': {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716cccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_answer = FinalAnswerTool()\n",
    "    model = HfApiModel(\n",
    "    max_tokens=2096,\n",
    "    temperature=0.5,\n",
    "    model_id='Qwen/Qwen2.5-Coder-32B-Instruct',\n",
    "    custom_role_conversions=None,\n",
    ")\n",
    "\n",
    "\n",
    "# Import tool from Hub\n",
    "image_generation_tool = load_tool(\"agents-course/text-to-image\", trust_remote_code=True)\n",
    "\n",
    "# Load system prompt from prompt.yaml file\n",
    "with open(\"prompts.yaml\", 'r') as stream:\n",
    "    prompt_templates = yaml.safe_load(stream)\n",
    "    \n",
    "agent = CodeAgent(\n",
    "    model=model,\n",
    "    tools=[final_answer], # add your tools here (don't remove final_answer)\n",
    "    max_steps=6,\n",
    "    verbosity_level=1,\n",
    "    grammar=None,\n",
    "    planning_interval=None,\n",
    "    name=None,\n",
    "    description=None,\n",
    "    prompt_templates=prompt_templates # Pass system prompt to CodeAgent\n",
    ")\n",
    "\n",
    "\n",
    "GradioUI(agent).launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
